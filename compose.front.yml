services:
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: bsllmner-mk2-api
    volumes:
      - ${PWD}:/app
      - /var/run/docker.sock:/var/run/docker.sock # For docker inspect
    environment:
      - BSLLMNER2_API_HOST=0.0.0.0
      - BSLLMNER2_API_PORT=8000
      - BSLLMNER2_DEBUG=true
      - OLLAMA_HOST=http://bsllmner-mk2-ollama:11434
    working_dir: /app
    entrypoint: [""]
    # command: ["bsllmner2_api"]
    command: ["sleep", "infinity"]
    restart: unless-stopped
    init: true
    networks:
      - bsllmner-mk2-network

  front:
    build:
      context: ./front
      dockerfile: Dockerfile
    container_name: bsllmner-mk2-front
    volumes:
      - ${PWD}/front:/app
      - node_modules:/app/node_modules
    environment:
      - BSLLMNER2_FRONT_HOST=0.0.0.0
      - BSLLMNER2_FRONT_PORT=3000
      - BSLLMNER2_FRONT_EXTERNAL_URL=http://localhost:3000 # Front URL as seen from the browser
      - BSLLMNER2_API_INTERNAL_URL=http://bsllmner-mk2-api:8000 # API URL as seen from the front container
      - BSLLMNER2_OLLAMA_URL=http://bsllmner-mk2-ollama:11434 # Ollama URL as seen from the front container
    ports:
      - 127.0.0.1:3000:3000
    working_dir: /app
    entrypoint: [""]
    # command: ["npm", "run", "preview"]
    command: [ "sleep", "infinity" ]
    restart: unless-stopped
    init: true
    networks:
      - bsllmner-mk2-network

  ollama:
    image: ollama/ollama:0.9.0
    container_name: bsllmner-mk2-ollama
    volumes:
      - ${PWD}/ollama-data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    entrypoint: ["/bin/ollama"]
    command: ["serve"]
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    init: true
    networks:
      - bsllmner-mk2-network

volumes:
  node_modules: {}

networks:
  bsllmner-mk2-network:
    name: bsllmner-mk2-network
    external: true
